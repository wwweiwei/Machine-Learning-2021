# HW1 - regularized linear model regression using LSE and Newton's method
### Required functions
- a. For LSE:
1. Use LU decomposition to find the inverse of (A_tranpose_A + lambda*I), Gauss-Jordan elimination will also be accepted. (A is the design matrix)
2. Print out the equation of the best fitting line and the error.
- b. For Newton's method:
1. Please use the method mentioned in the lesson.
2. Print out the equation of the best fitting line and the error, and compare to LSE.
- c. For visualization:
1. Please visualize the data points which are the input of program, and the best
fitting curve.
2. It's free to use any existing package.

### Notes
- lowerâ€“upper (LU) decomposition
    - factors a matrix as the product of a lower triangular matrix and an upper triangular matrix
    - LU decomposition can be viewed as the matrix form of Gaussian elimination.

### Reference
- LU decomposition: https://www.youtube.com/watch?v=64cajKdA0o4
- Newton's method: https://hackmd.io/@ierosodin/Machine_Learning/https%3A%2F%2Fhackmd.io%2Fs%2FB1_Bzdi5Q%23